{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b4309a6",
   "metadata": {},
   "source": [
    "# ğŸ” Retrieval Techniques - Beyond Simple Similarity Search\n",
    "\n",
    "> **Complete Guide with Code Explanations and Expected Outputs**\n",
    "\n",
    "This comprehensive notebook demonstrates four powerful retrieval techniques for finding relevant documents: **Basic Similarity Search**, **Sparse Retrieval (BM25)**, **Hybrid Search**, and **Maximum Marginal Relevance (MMR)**.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ› ï¸ Setup and Sample Data\n",
    "\n",
    "### ğŸ“¦ Required Libraries Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d517c218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\mahendra\\anaconda3\\envs\\youtube_env\\lib\\site-packages (4.1.0)\n",
      "Collecting rank-bm25\n",
      "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\mahendra\\anaconda3\\envs\\youtube_env\\lib\\site-packages (1.7.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\mahendra\\anaconda3\\envs\\youtube_env\\lib\\site-packages (2.2.6)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\mahendra\\anaconda3\\envs\\youtube_env\\lib\\site-packages (from sentence-transformers) (4.52.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mahendra\\anaconda3\\envs\\youtube_env\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\mahendra\\anaconda3\\envs\\youtube_env\\lib\\site-packages (from sentence-transformers) (2.7.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\mahendra\\anaconda3\\envs\\youtube_env\\lib\\site-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\mahendra\\anaconda3\\envs\\youtube_env\\lib\\site-packages (from sentence-transformers) (0.33.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\mahendra\\anaconda3\\envs\\youtube_env\\lib\\site-packages (from sentence-transformers) (11.2.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\mahendra\\anaconda3\\envs\\youtube_env\\lib\\site-packages (from sentence-transformers) (4.14.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\mahendra\\anaconda3\\envs\\youtube_env\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mahendra\\anaconda3\\envs\\youtube_env\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mahendra\\anaconda3\\envs\\youtube_env\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mahendra\\anaconda3\\envs\\youtube_env\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\mahendra\\anaconda3\\envs\\youtube_env\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\mahendra\\anaconda3\\envs\\youtube_env\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\mahendra\\anaconda3\\envs\\youtube_env\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\mahendra\\anaconda3\\envs\\youtube_env\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.5.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\mahendra\\anaconda3\\envs\\youtube_env\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\mahendra\\anaconda3\\envs\\youtube_env\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\mahendra\\anaconda3\\envs\\youtube_env\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\mahendra\\anaconda3\\envs\\youtube_env\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mahendra\\anaconda3\\envs\\youtube_env\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\mahendra\\anaconda3\\envs\\youtube_env\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\mahendra\\anaconda3\\envs\\youtube_env\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mahendra\\anaconda3\\envs\\youtube_env\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\mahendra\\anaconda3\\envs\\youtube_env\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mahendra\\anaconda3\\envs\\youtube_env\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mahendra\\anaconda3\\envs\\youtube_env\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mahendra\\anaconda3\\envs\\youtube_env\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.4.26)\n",
      "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
      "Installing collected packages: rank-bm25\n",
      "Successfully installed rank-bm25-0.2.2\n"
     ]
    }
   ],
   "source": [
    "! pip install sentence-transformers rank-bm25 scikit-learn numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bbef68",
   "metadata": {},
   "source": [
    "| Library | Purpose |\n",
    "|---------|---------|\n",
    "| `sentence-transformers` | Pre-trained embedding models for dense retrieval |\n",
    "| `rank-bm25` | Implementation of BM25 algorithm for sparse retrieval |\n",
    "| `scikit-learn` | Cosine similarity calculations |\n",
    "| `numpy` | Numerical operations |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1567a750",
   "metadata": {},
   "source": [
    "### ğŸ“„ Sample Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edf274d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"Machine learning algorithms are powerful tools for data analysis and prediction\",\n",
    "    \"Deep learning neural networks can process complex patterns in data\",\n",
    "    \"Python is a popular programming language for artificial intelligence\",\n",
    "    \"Data science involves extracting insights from large datasets\",\n",
    "    \"Natural language processing helps computers understand human language\",\n",
    "    \"Computer vision enables machines to interpret and analyze visual information\",\n",
    "    \"Supervised learning uses labeled data to train predictive models\",\n",
    "    \"Unsupervised learning finds hidden patterns in unlabeled data\",\n",
    "    \"Reinforcement learning trains agents through reward and punishment\",\n",
    "    \"Big data analytics requires specialized tools and techniques\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1779e7c0",
   "metadata": {},
   "source": [
    "#### ğŸ“‹ **Expected Output:**\n",
    "```\n",
    "Sample Documents:\n",
    "1. Machine learning algorithms are powerful tools for data analysis and prediction\n",
    "2. Deep learning neural networks can process complex patterns in data\n",
    "3. Python is a popular programming language for artificial intelligence\n",
    "4. Data science involves extracting insights from large datasets\n",
    "5. Natural language processing helps computers understand human language\n",
    "6. Computer vision enables machines to interpret and analyze visual information\n",
    "7. Supervised learning uses labeled data to train predictive models\n",
    "8. Unsupervised learning finds hidden patterns in unlabeled data\n",
    "9. Reinforcement learning trains agents through reward and punishment\n",
    "10. Big data analytics requires specialized tools and techniques\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 1ï¸âƒ£ Basic Similarity Search (Dense Retrieval)\n",
    "\n",
    "### ğŸ§  Code Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36122cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "doc_embeddings = model.encode(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc79ab5",
   "metadata": {},
   "source": [
    "**What's happening here:**\n",
    "- ğŸ”„ Loads a pre-trained sentence transformer model\n",
    "- ğŸ¯ Converts all documents into dense vector embeddings\n",
    "- ğŸ“Š Each document becomes a **384-dimensional vector**\n",
    "\n",
    "### ğŸ” The Search Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5f79505f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_similarity_search(query, top_k=3):\n",
    "    # Encode the input query into an embedding vector using the model\n",
    "    query_embedding = model.encode([query])\n",
    "    \n",
    "    # Compute cosine similarity between the query embedding and all document embeddings\n",
    "    similarities = cosine_similarity(query_embedding, doc_embeddings)[0]\n",
    "    \n",
    "    # Get the indices of the top_k most similar documents, sorted in descending order of similarity\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "    \n",
    "    # Prepare a list to store the final results\n",
    "    results = []\n",
    "    \n",
    "    # Iterate over the top indices and gather the corresponding scores and documents\n",
    "    for idx in top_indices:\n",
    "        results.append({\n",
    "            'score': similarities[idx],       # Similarity score between query and document\n",
    "            'document': documents[idx]        # The actual document text\n",
    "        })\n",
    "    \n",
    "    # Return the list of top_k results with scores and corresponding documents\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659bae74",
   "metadata": {},
   "source": [
    "#### âœ¨ **Expected Output for Query: \"machine learning algorithms\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e2488fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': np.float32(0.7338959),\n",
       "  'document': 'Machine learning algorithms are powerful tools for data analysis and prediction'},\n",
       " {'score': np.float32(0.45129114),\n",
       "  'document': 'Supervised learning uses labeled data to train predictive models'},\n",
       " {'score': np.float32(0.43556952),\n",
       "  'document': 'Deep learning neural networks can process complex patterns in data'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_similarity_search(\"machine learning algorithms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7426bcd0",
   "metadata": {},
   "source": [
    "ğŸ” Query: 'machine learning algorithms'\n",
    "\n",
    "ğŸ“Š Top 3 results:\n",
    "1. Score: 0.733 - Machine learning algorithms are powerful tools for data analysis and prediction\n",
    "2. Score: 0.451 - Supervised learning uses labeled data to train predictive models\n",
    "3. Score: 0.4355 - Deep learning neural networks can process complex patterns in data\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e46544b",
   "metadata": {},
   "source": [
    "\n",
    "> **ğŸ’¡ Why this works:** Dense embeddings capture semantic meaning, so phrases like *\"machine learning algorithms\"* can effectively match documents related to ML conceptsâ€”even if the exact keywords aren't present.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b331892b",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ Sparse Retrieval (BM25)\n",
    "\n",
    "### ğŸ“ Code Explanation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2209eb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "tokenized_docs = [doc.lower().split() for doc in documents]\n",
    "bm25 = BM25Okapi(tokenized_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c5b997",
   "metadata": {},
   "source": [
    "**Process breakdown:**\n",
    "- âœ‚ï¸ Tokenizes documents by splitting on whitespace and converting to lowercase\n",
    "- ğŸ—ï¸ Creates BM25 index for term frequency-based search\n",
    "- âš–ï¸ BM25 considers **term frequency**, **document frequency**, and **document length**\n",
    "\n",
    "### ğŸ¯ The Search Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2cb356ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_search(query, top_k=3):\n",
    "    # Convert the query to lowercase and split it into tokens (words)\n",
    "    query_tokens = query.lower().split()\n",
    "    \n",
    "    # Use the BM25 model to calculate relevance scores for each document based on the query tokens\n",
    "    scores = bm25.get_scores(query_tokens)\n",
    "    \n",
    "    # Get the indices of the top_k documents with the highest BM25 scores\n",
    "    top_indices = np.argsort(scores)[::-1][:top_k]\n",
    "    \n",
    "    # Prepare a list to store the final top_k results\n",
    "    results = []\n",
    "    \n",
    "    # Iterate over the top indices and collect the corresponding scores and documents\n",
    "    for idx in top_indices:\n",
    "        results.append({\n",
    "            'score': scores[idx],        # BM25 relevance score for the document\n",
    "            'document': documents[idx]   # The actual document content\n",
    "        })\n",
    "    \n",
    "    # Return the list of top_k results with scores and corresponding documents\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fa265b",
   "metadata": {},
   "source": [
    "#### âœ¨ **Expected Output for Query: \"machine learning algorithms\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34438bba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': np.float64(3.3372996537196844),\n",
       "  'document': 'Machine learning algorithms are powerful tools for data analysis and prediction'},\n",
       " {'score': np.float64(0.0),\n",
       "  'document': 'Big data analytics requires specialized tools and techniques'},\n",
       " {'score': np.float64(0.0),\n",
       "  'document': 'Reinforcement learning trains agents through reward and punishment'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_search(\"machine learning algorithms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabefddf",
   "metadata": {},
   "source": [
    "ğŸ” Query: 'machine learning algorithms'\n",
    "\n",
    "ğŸ“Š Top 3 Dense Embedding Results:\n",
    "1. Score: 3.337 â€” Machine learning algorithms are powerful tools for data analysis and prediction\n",
    "2. Score: 0.000 â€” Big data analytics requires specialized tools and techniques\n",
    "3. Score: 0.000 â€” Reinforcement learning trains agents through reward and punishment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98faf352",
   "metadata": {},
   "source": [
    "> **ğŸ’¡ Why this works:** BM25 gives high scores to documents containing the exact terms \"machine\", \"learning\", and \"algorithms\". It's excellent for keyword-based matching."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f0020a",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ Hybrid Search (Dense + Sparse)\n",
    "\n",
    "### ğŸ”„ Code Explanation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c1aa4096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_search(query, alpha=0.7, top_k=3):\n",
    "    \"\"\"\n",
    "    Perform a hybrid search combining dense (semantic) and sparse (BM25) scores.\n",
    "\n",
    "    Parameters:\n",
    "    - query (str): The search query.\n",
    "    - alpha (float): Weight for dense scores; (1 - alpha) is used for sparse scores.\n",
    "    - top_k (int): Number of top results to return.\n",
    "\n",
    "    Returns:\n",
    "    - List[dict]: Top documents with hybrid, dense, and sparse scores.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Dense retrieval using cosine similarity between query and document embeddings\n",
    "    query_embedding = model.encode([query])  # Encode the query into an embedding\n",
    "    dense_scores = cosine_similarity(query_embedding, doc_embeddings)[0]  # Compute dense similarity scores\n",
    "\n",
    "    # Step 2: Sparse retrieval using BM25 (bag-of-words based relevance)\n",
    "    query_tokens = query.lower().split()  # Tokenize the query\n",
    "    sparse_scores = bm25.get_scores(query_tokens)  # Compute BM25 scores for the query\n",
    "\n",
    "    # Step 3: Normalize both dense and sparse scores to the 0â€“1 range\n",
    "    dense_norm = (dense_scores - dense_scores.min()) / (dense_scores.max() - dense_scores.min() + 1e-8)\n",
    "    sparse_norm = (sparse_scores - sparse_scores.min()) / (sparse_scores.max() - sparse_scores.min() + 1e-8)\n",
    "\n",
    "    # Step 4: Combine both scores using a weighted average controlled by alpha\n",
    "    hybrid_scores = alpha * dense_norm + (1 - alpha) * sparse_norm  # Final score balances semantic and lexical relevance\n",
    "\n",
    "    # Step 5: Identify indices of the top_k documents with the highest hybrid scores\n",
    "    top_indices = np.argsort(hybrid_scores)[::-1][:top_k]\n",
    "\n",
    "    # Step 6: Build the final list of top results with relevant metadata\n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        results.append({\n",
    "            \"rank\": len(results) + 1,             # Rank of the document\n",
    "            \"document\": documents[idx],           # The actual document text\n",
    "            \"hybrid_score\": float(hybrid_scores[idx]),  # Combined score\n",
    "            \"dense_score\": float(dense_norm[idx]),      # Normalized dense score\n",
    "            \"sparse_score\": float(sparse_norm[idx]),    # Normalized sparse score\n",
    "        })\n",
    "\n",
    "    # Return the list of results\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155a7aef",
   "metadata": {},
   "source": [
    "**Process steps:**\n",
    "1. ğŸ”„ Runs both dense and sparse retrieval\n",
    "2. ğŸ“ Normalizes scores to comparable ranges (0-1)\n",
    "3. âš–ï¸ Combines using weighted average where **alpha** controls the balance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8581c372",
   "metadata": {},
   "source": [
    "#### **Alpha = 0.7** (70% dense, 30% sparse):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "156ab5fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rank': 1,\n",
       "  'document': 'Machine learning algorithms are powerful tools for data analysis and prediction',\n",
       "  'hybrid_score': 0.9999999871801407,\n",
       "  'dense_score': 1.0,\n",
       "  'sparse_score': 0.9999999970035655},\n",
       " {'rank': 2,\n",
       "  'document': 'Supervised learning uses labeled data to train predictive models',\n",
       "  'hybrid_score': 0.27922937273979187,\n",
       "  'dense_score': 0.398899108171463,\n",
       "  'sparse_score': 0.0},\n",
       " {'rank': 3,\n",
       "  'document': 'Deep learning neural networks can process complex patterns in data',\n",
       "  'hybrid_score': 0.2558214068412781,\n",
       "  'dense_score': 0.3654591739177704,\n",
       "  'sparse_score': 0.0}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrid_search(\"machine learning algorithms\", alpha=0.7, top_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef9f852",
   "metadata": {},
   "source": [
    "ğŸ“Š Top 3 hybrid results:\n",
    "1. Hybrid: 0.999 (Dense: 1.0, Sparse: 0.999) - Machine learning algorithms are powerful tools for data analysis and prediction\n",
    "2. Hybrid: 0.279 (Dense: 0.398, Sparse: 0.0) - Supervised learning uses labeled data to train predictive models\n",
    "3. Hybrid: 0.255 (Dense: 0.365, Sparse: 0.0) - Deep learning neural networks can process complex patterns in data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0cfd18",
   "metadata": {},
   "source": [
    "#### **Alpha = 0.3** (30% dense, 70% sparse):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c25bccb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rank': 1,\n",
       "  'document': 'Machine learning algorithms are powerful tools for data analysis and prediction',\n",
       "  'hybrid_score': 1.0000000098234247,\n",
       "  'dense_score': 1.0,\n",
       "  'sparse_score': 0.9999999970035655},\n",
       " {'rank': 2,\n",
       "  'document': 'Supervised learning uses labeled data to train predictive models',\n",
       "  'hybrid_score': 0.11966973543167114,\n",
       "  'dense_score': 0.398899108171463,\n",
       "  'sparse_score': 0.0},\n",
       " {'rank': 3,\n",
       "  'document': 'Deep learning neural networks can process complex patterns in data',\n",
       "  'hybrid_score': 0.10963775962591171,\n",
       "  'dense_score': 0.3654591739177704,\n",
       "  'sparse_score': 0.0}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrid_search(\"machine learning algorithms\", alpha=0.3, top_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21311522",
   "metadata": {},
   "source": [
    "ğŸ“Š Top 3 hybrid results:\n",
    "1. Hybrid: 1.000 (Dense: 1.0, Sparse: 0.999) - Machine learning algorithms are powerful tools for data analysis and prediction\n",
    "2. Hybrid: 0.119 (Dense: 0.398, Sparse: 0.0) - Supervised learning uses labeled data to train predictive models\n",
    "3. Hybrid: 0.109 (Dense: 0.365, Sparse: 0.0) - Unsupervised learning finds hidden patterns in unlabeled data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d5d99f",
   "metadata": {},
   "source": [
    "> **ğŸ’¡ Why this works:** Hybrid search combines the semantic understanding of dense retrieval with the precise keyword matching of sparse retrieval, often providing the best of both worlds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8d1c48",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ Maximum Marginal Relevance (MMR)\n",
    "\n",
    "### ğŸ¯ Code Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f3a3db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mmr_search(query, lambda_param=0.7, top_k=3):\n",
    "    \"\"\"\n",
    "    Perform Maximal Marginal Relevance (MMR) based search for diverse and relevant documents.\n",
    "\n",
    "    Parameters:\n",
    "    - query (str): The user query.\n",
    "    - lambda_param (float): Trade-off between relevance and diversity (0 to 1).\n",
    "    - top_k (int): Number of top documents to return.\n",
    "\n",
    "    Returns:\n",
    "    - List[dict]: Top documents with metadata including scores.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Embed the input query using the model to get its dense vector representation\n",
    "    query_embedding = model.encode([query])\n",
    "    \n",
    "    # Compute cosine similarity between the query and all document embeddings\n",
    "    relevance_scores = cosine_similarity(query_embedding, doc_embeddings)[0]\n",
    "\n",
    "    # Initialize lists to track selected and unselected document indices\n",
    "    selected_indices = []\n",
    "    remaining_indices = list(range(len(documents)))\n",
    "\n",
    "    # Step 2: Select the most relevant document (highest cosine similarity)\n",
    "    first_idx = np.argmax(relevance_scores)  # Index of the most relevant document\n",
    "    selected_indices.append(first_idx)       # Add it to selected list\n",
    "    remaining_indices.remove(first_idx)      # Remove it from remaining list\n",
    "\n",
    "    # Step 3: Iteratively select the rest of the top_k documents using MMR\n",
    "    for _ in range(top_k - 1):\n",
    "        mmr_scores = []  # List to store MMR scores for each candidate document\n",
    "\n",
    "        for idx in remaining_indices:\n",
    "            relevance = relevance_scores[idx]  # Relevance score of the current document\n",
    "\n",
    "            # Calculate max similarity to already selected documents to ensure diversity\n",
    "            max_sim_to_selected = max(\n",
    "                cosine_similarity(\n",
    "                    doc_embeddings[idx].reshape(1, -1),\n",
    "                    doc_embeddings[sel_idx].reshape(1, -1)\n",
    "                )[0][0] for sel_idx in selected_indices\n",
    "            )\n",
    "\n",
    "            # Apply the MMR formula: trade-off between relevance and diversity\n",
    "            mmr_score = lambda_param * relevance - (1 - lambda_param) * max_sim_to_selected\n",
    "\n",
    "            # Append index and its MMR score\n",
    "            mmr_scores.append((idx, mmr_score))\n",
    "\n",
    "        # Select the document with the highest MMR score\n",
    "        best_idx = max(mmr_scores, key=lambda x: x[1])[0]\n",
    "        selected_indices.append(best_idx)        # Add to selected\n",
    "        remaining_indices.remove(best_idx)       # Remove from remaining\n",
    "\n",
    "    # Step 4: Build the final result list with ranks, documents, and scores\n",
    "    results = []\n",
    "    for rank, idx in enumerate(selected_indices, 1):\n",
    "        results.append({\n",
    "            \"rank\": rank,                            # Rank of the document\n",
    "            \"document\": documents[idx],              # The document text\n",
    "            \"relevance_score\": float(relevance_scores[idx]),  # Original relevance score\n",
    "            \"index\": idx                             # Index of the document in original list\n",
    "        })\n",
    "\n",
    "    # Return the list of selected top_k documents with metadata\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034c6aee",
   "metadata": {},
   "source": [
    "\n",
    "**Algorithm steps:**\n",
    "1. ğŸ¯ First selects the most relevant document\n",
    "2. âš–ï¸ For each subsequent selection, balances relevance against similarity to already-selected documents\n",
    "3. ğŸšï¸ Lambda parameter controls **relevance vs diversity** trade-off\n",
    "\n",
    "### âœ¨ **Expected Output for Query: \"machine learning\"**\n",
    "\n",
    "#### **Lambda = 0.7** (70% relevance, 30% diversity):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "afda3550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rank': 1,\n",
       "  'document': 'Machine learning algorithms are powerful tools for data analysis and prediction',\n",
       "  'relevance_score': 0.6522308588027954,\n",
       "  'index': np.int64(0)},\n",
       " {'rank': 2,\n",
       "  'document': 'Supervised learning uses labeled data to train predictive models',\n",
       "  'relevance_score': 0.4839949309825897,\n",
       "  'index': 6},\n",
       " {'rank': 3,\n",
       "  'document': 'Deep learning neural networks can process complex patterns in data',\n",
       "  'relevance_score': 0.41273051500320435,\n",
       "  'index': 1}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmr_search(\"machine learning\", lambda_param=0.7, top_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d053aa7b",
   "metadata": {},
   "source": [
    "ğŸ” **MMR Search** â€” Query: *\"machine learning\"* (Î» = 0.7)\n",
    "\n",
    "1. ğŸ¯ **Rank 1** (Most Relevant):  \n",
    "   *Machine learning algorithms are powerful tools for data analysis and prediction*  \n",
    "   **Relevance Score:** 0.652\n",
    "\n",
    "2. ğŸ”„ **Rank 2** (MMR Selected for Diversity):  \n",
    "   *Supervised learning uses labeled data to train predictive models*  \n",
    "   **Relevance Score:** 0.484\n",
    "\n",
    "3. ğŸ”„ **Rank 3** (MMR Selected for Diversity):  \n",
    "   *Deep learning neural networks can process complex patterns in data*  \n",
    "   **Relevance Score:** 0.413\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256c2d48",
   "metadata": {},
   "source": [
    "#### **Lambda = 0.3** (30% relevance, 70% diversity):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "56a3a9f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rank': 1,\n",
       "  'document': 'Machine learning algorithms are powerful tools for data analysis and prediction',\n",
       "  'relevance_score': 0.6522308588027954,\n",
       "  'index': np.int64(0)},\n",
       " {'rank': 2,\n",
       "  'document': 'Reinforcement learning trains agents through reward and punishment',\n",
       "  'relevance_score': 0.278318852186203,\n",
       "  'index': 8},\n",
       " {'rank': 3,\n",
       "  'document': 'Unsupervised learning finds hidden patterns in unlabeled data',\n",
       "  'relevance_score': 0.3412044644355774,\n",
       "  'index': 7}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmr_search(\"machine learning\", lambda_param=0.3, top_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdae76e",
   "metadata": {},
   "source": [
    "ğŸ” **MMR Search** â€” Query: *\"machine learning\"* (Î» = 0.3)\n",
    "\n",
    "1. ğŸ¯ **Rank 1** (Most Relevant):  \n",
    "   *Machine learning algorithms are powerful tools for data analysis and prediction*  \n",
    "   **Relevance Score:** 0.652\n",
    "\n",
    "2. ğŸŒŸ **Rank 2** (MMR Selected for Diversity):  \n",
    "   *Reinforcement learning trains agents through reward and punishment*  \n",
    "   **Relevance Score:** 0.278\n",
    "\n",
    "3. ğŸŒŸ **Rank 3** (MMR Selected for Diversity):  \n",
    "   *Unsupervised learning finds hidden patterns in unlabeled data*  \n",
    "   **Relevance Score:** 0.341\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192ae8df",
   "metadata": {},
   "source": [
    "> **ğŸ’¡ Why this works:** MMR prevents returning multiple very similar documents. With higher lambda, you get more relevant but potentially similar results. With lower lambda, you get more diverse results that cover different aspects of the topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960d9abf",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ Method Comparison\n",
    "### ğŸ” **Expected Output for Query: \"machine learning\"**\n",
    "\n",
    "<table>\n",
    "<tr><th>Method</th><th>Results</th></tr>\n",
    "\n",
    "<tr>\n",
    "<td><strong>ğŸ§  DENSE SIMILARITY</strong></td>\n",
    "<td>\n",
    "<pre>\n",
    "1. Score: 0.734 - Machine learning algorithms are powerful tools for data analysis and prediction  \n",
    "2. Score: 0.451 - Supervised learning uses labeled data to train predictive models  \n",
    "3. Score: 0.436 - Deep learning neural networks can process complex patterns in data  \n",
    "</pre>\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td><strong>ğŸ“ SPARSE (BM25)</strong></td>\n",
    "<td>\n",
    "<pre>\n",
    "1. Score: 3.337 - Machine learning algorithms are powerful tools for data analysis and prediction  \n",
    "2. Score: 0.000 - Big data analytics requires specialized tools and techniques  \n",
    "3. Score: 0.000 - Reinforcement learning trains agents through reward and punishment  \n",
    "</pre>\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td><strong>ğŸ”„ HYBRID</strong></td>\n",
    "<td>\n",
    "<pre>\n",
    "1. Hybrid: 1.000 (Dense: 1.000, Sparse: 1.000) - Machine learning algorithms are powerful tools for data analysis and prediction  \n",
    "2. Hybrid: 0.279 (Dense: 0.399, Sparse: 0.000) - Supervised learning uses labeled data to train predictive models  \n",
    "3. Hybrid: 0.256 (Dense: 0.365, Sparse: 0.000) - Deep learning neural networks can process complex patterns in data  \n",
    "</pre>\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td><strong>ğŸ¯ MMR</strong></td>\n",
    "<td>\n",
    "<pre>\n",
    "1. Selected (relevance: 0.652): Machine learning algorithms are powerful tools for data analysis and prediction  \n",
    "2. Selected (relevance: 0.484): Supervised learning uses labeled data to train predictive models  \n",
    "3. Selected (relevance: 0.413): Deep learning neural networks can process complex patterns in data  \n",
    "</pre>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0752dfda",
   "metadata": {},
   "source": [
    "## ğŸ“Š Key Insights and When to Use Each Method\n",
    "\n",
    "### ğŸš€ Performance Characteristics\n",
    "\n",
    "<table>\n",
    "<tr><th>Method</th><th>Strengths</th><th>Weaknesses</th><th>Best Use Cases</th></tr>\n",
    "<tr>\n",
    "<td><strong>ğŸ§  Basic Similarity</strong></td>\n",
    "<td>âœ… Fast and simple<br>âœ… Good semantic understanding</td>\n",
    "<td>âŒ May miss exact keyword matches</td>\n",
    "<td>Quick prototyping, semantic search primary</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><strong>ğŸ“ Sparse (BM25)</strong></td>\n",
    "<td>âœ… Excellent keyword matching<br>âœ… Works well with technical terms</td>\n",
    "<td>âŒ Poor semantic understanding</td>\n",
    "<td>Legal documents, technical specs, exact terms</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><strong>ğŸ”„ Hybrid Search</strong></td>\n",
    "<td>âœ… Best overall performance<br>âœ… Semantic + keyword matching</td>\n",
    "<td>âŒ More complex to tune</td>\n",
    "<td>Production systems, balanced requirements</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><strong>ğŸ¯ MMR</strong></td>\n",
    "<td>âœ… Prevents redundant results<br>âœ… Good for exploration</td>\n",
    "<td>âŒ May sacrifice relevance for diversity</td>\n",
    "<td>Research, content discovery, avoiding redundancy</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845e77c7",
   "metadata": {},
   "source": [
    "### âš™ï¸ Parameter Tuning Guidelines\n",
    "\n",
    "#### ğŸ”„ **Hybrid Search Alpha Values:**\n",
    "| Alpha Range | Focus | Use Case |\n",
    "|-------------|-------|----------|\n",
    "| `Î± = 0.8-0.9` | ğŸ§  Semantic-heavy | Research papers, technical documentation |\n",
    "| `Î± = 0.5-0.6` | ğŸ“ Keyword-heavy | Legal, compliance, exact specifications |\n",
    "| `Î± = 0.7` | âš–ï¸ Balanced | General-purpose applications |\n",
    "\n",
    "#### ğŸ¯ **MMR Lambda Values:**\n",
    "| Lambda Range | Focus | Use Case |\n",
    "|-------------|-------|----------|\n",
    "| `Î» = 0.8-0.9` | ğŸ¯ Relevance-focused | Specific information needs |\n",
    "| `Î» = 0.5-0.6` | ğŸŒŸ Diversity-focused | Brainstorming, exploration |\n",
    "| `Î» = 0.7` | âš–ï¸ Balanced | Most common use case |\n",
    "\n",
    "### ğŸŒ Real-World Applications\n",
    "\n",
    "| Application | Recommended Method | Reasoning |\n",
    "|-------------|-------------------|-----------|\n",
    "| ğŸ” **Search Engines** | Hybrid + MMR | Diverse, relevant results |\n",
    "| ğŸ“‹ **Document Q&A** | Basic Similarity | Semantic matching priority |\n",
    "| âš–ï¸ **Legal Research** | BM25 | Exact term matching critical |\n",
    "| ğŸ“º **Content Recommendation** | MMR | Diverse suggestions needed |\n",
    "| ğŸ“– **Technical Documentation** | Hybrid (high Î±) | Semantic understanding important |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb458f81",
   "metadata": {},
   "source": [
    "## ğŸ Conclusion\n",
    "\n",
    "This comprehensive guide provides the foundation for implementing sophisticated retrieval systems that go beyond simple similarity search. Each method has its strengths and optimal use cases:\n",
    "\n",
    "- **Start with Basic Similarity** for quick prototypes\n",
    "- **Use BM25** when exact keywords matter most\n",
    "- **Implement Hybrid Search** for production systems\n",
    "- **Add MMR** when diversity is important\n",
    "\n",
    "> **ğŸš€ Pro Tip:** Most production systems benefit from a hybrid approach with MMR post-processing to balance relevance, precision, and diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd0a708",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Youtube_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
