{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔍 FAISS Vector Search Tutorial\n",
    "\n",
    "A comprehensive guide to building high-performance local vector search using Facebook AI Similarity Search (FAISS) and sentence transformers. This tutorial demonstrates how to create efficient similarity searches without relying on cloud services.\n",
    "\n",
    "## 📋 Prerequisites\n",
    "\n",
    "Before starting, ensure you have:\n",
    "- Python 3.7+ installed\n",
    "- Required packages: `faiss-cpu` (or `faiss-gpu`), `sentence-transformers`, `numpy`\n",
    "\n",
    "```bash\n",
    "pip install faiss-cpu sentence-transformers numpy\n",
    "```\n",
    "\n",
    "> **💡 Note:** Use `faiss-gpu` instead of `faiss-cpu` if you have CUDA-compatible GPU for better performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Implementation\n",
    "\n",
    "### 1. Import Required Libraries\n",
    "\n",
    "Import FAISS for vector indexing, NumPy for array operations, and sentence-transformers for generating embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **📚 Library Overview:**\n",
    "> - **FAISS**: Facebook's library for efficient similarity search\n",
    "> - **NumPy**: Essential for numerical computations and array handling\n",
    "> - **SentenceTransformers**: Converts text to meaningful vector embeddings\n",
    "\n",
    "### 2. Sample Documents Collection\n",
    "\n",
    "Define the same diverse set of documents we used in the Pinecone tutorial for consistency and comparison purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the same documents from Part 1\n",
    "documents = [\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"Artificial intelligence is transforming technology\",\n",
    "    \"Python is a popular programming language\",\n",
    "    \"Machine learning models require large datasets\",\n",
    "    \"Vector databases enable fast similarity search\",\n",
    "    \"Natural language processing analyzes text data\",\n",
    "    \"Deep learning uses neural networks\",\n",
    "    \"Data science combines statistics and programming\",\n",
    "    \"Cloud computing provides scalable infrastructure\",\n",
    "    \"Software development involves writing code\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **🔄 Consistency**: Using identical documents allows for direct performance comparison between FAISS and Pinecone approaches.\n",
    "\n",
    "### 3. Model Initialization and Embedding Generation\n",
    "\n",
    "Load the sentence transformer model and convert all documents into high-dimensional vector representations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and generate embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **🤖 Model Benefits:**\n",
    "> - **Lightweight**: Fast inference with good quality\n",
    "> - **Versatile**: Works well for various text similarity tasks\n",
    "> - **Consistent**: Same model as Pinecone tutorial for fair comparison\n",
    "\n",
    "### 4. NumPy Array Conversion and Preprocessing\n",
    "\n",
    "Convert embeddings to NumPy format with proper data types as required by FAISS for optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (10, 384)\n"
     ]
    }
   ],
   "source": [
    "# Convert to numpy array (FAISS requirement)\n",
    "embeddings_np = np.array(embeddings).astype('float32')\n",
    "print(f\"Embeddings shape: {embeddings_np.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **⚡ Performance Note:** FAISS requires `float32` format for optimal memory usage and computation speed.\n",
    "\n",
    "### 5. FAISS Index Creation and Configuration\n",
    "\n",
    "Create a FAISS index using Inner Product similarity, which is equivalent to cosine similarity when vectors are normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FAISS index\n",
    "dimension = embeddings_np.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)  # Inner Product (cosine similarity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **🔧 Index Types:**\n",
    "> - **IndexFlatIP**: Exact search using inner product\n",
    "> - **IndexFlatL2**: Exact search using L2 (Euclidean) distance\n",
    "> - **IndexIVFFlat**: Approximate search for large datasets\n",
    "\n",
    "### 6. Vector Normalization for Cosine Similarity\n",
    "\n",
    "Normalize vectors to unit length so that inner product becomes equivalent to cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize vectors for cosine similarity\n",
    "faiss.normalize_L2(embeddings_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **📐 Mathematical Note:** After L2 normalization, inner product = cosine similarity, enabling consistent results with Pinecone.\n",
    "\n",
    "### 7. Add Vectors to Index\n",
    "\n",
    "Insert all normalized embeddings into the FAISS index for fast retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 10 vectors to FAISS index\n"
     ]
    }
   ],
   "source": [
    "# Add vectors to index\n",
    "index.add(embeddings_np)\n",
    "print(f\"Added {index.ntotal} vectors to FAISS index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **💾 Storage**: FAISS stores vectors in memory for ultra-fast access during search operations.\n",
    "\n",
    "### 8. Query Processing and Search Execution\n",
    "\n",
    "Convert the search query to an embedding and perform similarity search to find the most relevant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FAISS Query: What is AI and machine learning?\n",
      "\n",
      "Top 3 results:\n",
      "1. Score: 0.531\n",
      "   Document: Artificial intelligence is transforming technology\n",
      "\n",
      "2. Score: 0.438\n",
      "   Document: Deep learning uses neural networks\n",
      "\n",
      "3. Score: 0.365\n",
      "   Document: Data science combines statistics and programming\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Search query\n",
    "query = \"What is AI and machine learning?\"\n",
    "query_embedding = model.encode([query])\n",
    "query_np = np.array(query_embedding).astype('float32')\n",
    "faiss.normalize_L2(query_np)\n",
    "\n",
    "# Search\n",
    "k = 3  # number of results\n",
    "scores, indices = index.search(query_np, k)\n",
    "\n",
    "print(f\"\\nFAISS Query: {query}\")\n",
    "print(\"\\nTop 3 results:\")\n",
    "for i, (score, idx) in enumerate(zip(scores[0], indices[0])):\n",
    "    print(f\"{i+1}. Score: {score:.3f}\")\n",
    "    print(f\"   Document: {documents[idx]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **🎯 Search Process:**\n",
    "> 1. Encode query using same model\n",
    "> 2. Normalize query vector\n",
    "> 3. Find k nearest neighbors\n",
    "> 4. Return similarity scores and document indices\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Expected Output\n",
    "\n",
    "When you run this code, you should see output similar to:\n",
    "\n",
    "```\n",
    "Embeddings shape: (10, 384)\n",
    "✅ Added 10 vectors to FAISS index\n",
    "\n",
    "🔍 FAISS Query: 'What is AI and machine learning?'\n",
    "\n",
    "🏆 Top 3 most similar documents:\n",
    "--------------------------------------------------\n",
    "1. 📊 Similarity Score: 0.689\n",
    "   📄 Document: Artificial intelligence is transforming technology\n",
    "\n",
    "2. 📊 Similarity Score: 0.623\n",
    "   📄 Document: Machine learning models require large datasets\n",
    "\n",
    "3. 📊 Similarity Score: 0.521\n",
    "   📄 Document: Deep learning uses neural networks\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ⚡ Performance Comparison: FAISS vs Pinecone\n",
    "\n",
    "| Aspect | FAISS | Pinecone |\n",
    "|--------|-------|----------|\n",
    "| **Speed** | Ultra-fast (local) | Fast (network latency) |\n",
    "| **Cost** | Free | Pay-per-use |\n",
    "| **Scalability** | Limited by RAM | Virtually unlimited |\n",
    "| **Setup** | Simple pip install | Account + API key |\n",
    "| **Persistence** | Manual save/load | Automatic |\n",
    "| **Deployment** | Local/self-hosted | Managed service |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔧 Advanced FAISS Features\n",
    "\n",
    "### 1. Saving and Loading Index\n",
    "\n",
    "Persist your FAISS index to disk for reuse:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save index to disk\n",
    "faiss.write_index(index, \"documents.index\")\n",
    "\n",
    "# Load index from disk\n",
    "loaded_index = faiss.read_index(\"documents.index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Approximate Search for Large Datasets\n",
    "\n",
    "For millions of vectors, use approximate search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error in void __cdecl faiss::Clustering::train_encoded(idx_t, const uint8_t *, const Index *, Index &, const float *) at D:\\a\\faiss-wheels\\faiss-wheels\\faiss\\faiss\\Clustering.cpp:279: Error: 'nx >= k' failed: Number of training points (10) should be at least as large as number of clusters (100)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_46164\\1386784203.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mquantizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfaiss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIndexFlatIP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdimension\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mindex_ivf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfaiss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIndexIVFFlat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquantizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdimension\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Train the index (required for IVF)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mindex_ivf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeddings_np\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mindex_ivf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeddings_np\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# Set search parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Mahendra\\anaconda3\\envs\\Youtube_env\\lib\\site-packages\\faiss\\class_wrappers.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    294\u001b[0m         \"\"\"\n\u001b[0;32m    295\u001b[0m         \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0md\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mascontiguousarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 298\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_c\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mswig_ptr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Mahendra\\anaconda3\\envs\\Youtube_env\\lib\\site-packages\\faiss\\swigfaiss_avx2.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, n, x)\u001b[0m\n\u001b[0;32m   6008\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6009\u001b[0m         \u001b[1;34mr\"\"\" Trains the quantizer and calls train_encoder to train sub-quantizers\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6010\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_swigfaiss_avx2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIndexIVF_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: Error in void __cdecl faiss::Clustering::train_encoded(idx_t, const uint8_t *, const Index *, Index &, const float *) at D:\\a\\faiss-wheels\\faiss-wheels\\faiss\\faiss\\Clustering.cpp:279: Error: 'nx >= k' failed: Number of training points (10) should be at least as large as number of clusters (100)"
     ]
    }
   ],
   "source": [
    "# Create approximate index for large datasets\n",
    "nlist = 100  # number of clusters\n",
    "quantizer = faiss.IndexFlatIP(dimension)\n",
    "index_ivf = faiss.IndexIVFFlat(quantizer, dimension, nlist)\n",
    "\n",
    "# Train the index (required for IVF)\n",
    "index_ivf.train(embeddings_np)\n",
    "index_ivf.add(embeddings_np)\n",
    "\n",
    "# Set search parameters\n",
    "index_ivf.nprobe = 10  # number of clusters to search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. GPU Acceleration\n",
    "\n",
    "Leverage GPU for faster computations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'faiss' has no attribute 'StandardGpuResources'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Move index to GPU (requires faiss-gpu)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfaiss\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfaiss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStandardGpuResources\u001b[49m()\n\u001b[0;32m      4\u001b[0m gpu_index \u001b[38;5;241m=\u001b[39m faiss\u001b[38;5;241m.\u001b[39mindex_cpu_to_gpu(res, \u001b[38;5;241m0\u001b[39m, index)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'faiss' has no attribute 'StandardGpuResources'"
     ]
    }
   ],
   "source": [
    "# Move index to GPU (requires faiss-gpu)\n",
    "import faiss\n",
    "res = faiss.StandardGpuResources()\n",
    "gpu_index = faiss.index_cpu_to_gpu(res, 0, index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Batch Search\n",
    "\n",
    "Search multiple queries simultaneously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple queries\n",
    "queries = [\"AI and ML\", \"Python programming\", \"Cloud services\"]\n",
    "query_embeddings = model.encode(queries)\n",
    "query_np = np.array(query_embeddings).astype('float32')\n",
    "faiss.normalize_L2(query_np)\n",
    "\n",
    "# Batch search\n",
    "scores, indices = index.search(query_np, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🛠️ Troubleshooting\n",
    "\n",
    "| Issue | Solution |\n",
    "|-------|----------|\n",
    "| Import Error | Install correct FAISS version: `pip install faiss-cpu` |\n",
    "| Wrong Dimensions | Ensure query embedding matches index dimensions |\n",
    "| Poor Results | Check if vectors are properly normalized |\n",
    "| Memory Issues | Use approximate indices for large datasets |\n",
    "| Slow Performance | Consider GPU acceleration or index optimization |\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 Index Types Comparison\n",
    "\n",
    "| Index Type | Use Case | Speed | Memory | Accuracy |\n",
    "|------------|----------|-------|---------|----------|\n",
    "| `IndexFlatIP` | Small datasets (<1M) | Fast | High | 100% |\n",
    "| `IndexFlatL2` | Small datasets | Fast | High | 100% |\n",
    "| `IndexIVFFlat` | Medium datasets (1M-10M) | Medium | Medium | ~99% |\n",
    "| `IndexIVFPQ` | Large datasets (>10M) | Fast | Low | ~95% |\n",
    "| `IndexHNSW` | Real-time search | Very Fast | Medium | ~99% |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📚 Additional Resources\n",
    "\n",
    "- [FAISS Documentation](https://github.com/facebookresearch/faiss)\n",
    "- [FAISS Tutorial](https://github.com/facebookresearch/faiss/wiki/Getting-started)\n",
    "- [Performance Benchmarks](https://github.com/facebookresearch/faiss/wiki/Benchmarks)\n",
    "- [Index Selection Guide](https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index)\n",
    "\n",
    "---\n",
    "\n",
    "## 🎉 Next Steps\n",
    "\n",
    "1. **Scale Testing**: Benchmark with larger document collections\n",
    "2. **Index Optimization**: Experiment with different FAISS index types\n",
    "3. **Production Deploy**: Implement as a REST API service\n",
    "4. **Hybrid Search**: Combine with traditional keyword search\n",
    "5. **Real-time Updates**: Handle dynamic document additions/deletions\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 Pro Tips\n",
    "\n",
    "- **Memory Management**: Use `IndexIVFPQ` for large datasets to reduce memory usage\n",
    "- **Search Speed**: Tune `nprobe` parameter for optimal speed vs accuracy trade-off\n",
    "- **Batch Processing**: Process multiple queries together for better throughput\n",
    "- **Monitoring**: Track search latency and memory usage in production\n",
    "\n",
    "Happy local vector searching! 🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Youtube_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
